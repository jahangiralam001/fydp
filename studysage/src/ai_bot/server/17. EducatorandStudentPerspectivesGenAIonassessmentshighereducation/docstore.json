[[["0ecc71d7-39f3-4dde-a5b7-f50be50989c5",{"pageContent":"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/372482047\nEducator and Student Perspectives on the Impact of Generative AI on\nAssessments in Higher Education\nConference Paper · July 2023\nDOI: 10.1145/3573051.3596191\nCITATIONS\n2\nREADS\n69\n6 authors, including:\nSome of the authors of this publication are also working on these related projects:\nSelf-regulated Learning in MOOCs View project\nConnected Learning at Scale (CLaS) View project\nAdele Smolansky\nCornell University\n3 PUBLICATIONS   2 CITATIONS   \nSEE PROFILE\nSandris Zeivots\nThe University of Sydney\n21 PUBLICATIONS   79 CITATIONS   \nSEE PROFILE\nElaine Huber\nThe University of Sydney\n43 PUBLICATIONS   140 CITATIONS   \nSEE PROFILE\nRené F Kizilcec\nCornell University\n93 PUBLICATIONS   4,949 CITATIONS   \nSEE PROFILE\nAll content following this page was uploaded by Sandris Zeivots on 04 September 2023.\nThe user has requested enhancement of the downloaded file.","metadata":{"id":0}}],["d2852896-b99a-4d7b-afc3-86777396ef65",{"pageContent":"93 PUBLICATIONS   4,949 CITATIONS   \nSEE PROFILE\nAll content following this page was uploaded by Sandris Zeivots on 04 September 2023.\nThe user has requested enhancement of the downloaded file.\n   \n \n   \n \nEducator and Student Perspectives on the Impact of Generative \nAI on Assessments in Higher Education \nAdele Smolansky \nDepartment of Computer Science \nCornell University \nIthaca, NY, USA \nas2953@cornell.edu \nSandris Zeivots \nBusiness Co-Design \nThe University of Sydney \nBusiness School  \nSydney, NSW, Australia \nsandris.zeivots@sydney.edu.au  \nAndrew Cram \nBusiness Co-Design \nThe University of Sydney Business \nSchool \nSydney, NSW, Australia \n andrew.cram@sydney.edu.au \nElaine Huber \nBusiness Co-Design \nThe University of Sydney Business \nSchool \nSydney, NSW, Australia \nelaine.huber@sydney.edu.au \nCorina Raduescu \nBusiness Information Systems \nThe University of Sydney \nBusiness School  \nSydney, NSW, Australia \ncorina.raduescu@sydney.edu.au  \nRené F. Kizilcec \nDepartment of Information \nScience \nCornell University","metadata":{"id":1}}],["d0378b7c-2dec-428b-8c83-53b8c46c36e8",{"pageContent":"Business Information Systems \nThe University of Sydney \nBusiness School  \nSydney, NSW, Australia \ncorina.raduescu@sydney.edu.au  \nRené F. Kizilcec \nDepartment of Information \nScience \nCornell University \nIthaca, NY, USA \nkizilcec@cornell.edu\n \nABSTRACT \nThe sudden popularity and availability of generative AI tools, such \nas ChatGPT that can write compelling essays on any topic, code in \nvarious programming languages, and ace standardized tests across \ndomains, raises questions about  the  sustainability  of  traditional \nassessment  practices. To  seize  this opportunity for innovation in \nassessment practice, we conducted a survey to understand both the \neducators’ and students’ perspectives on the issue. We measure and \ncompare  attitudes  of  both  stakeholders across  various  assessment \nscenarios, building on an established framework for examining the \nquality of online assessments along six dimensions. Responses from \n389  students  and  36  educators across  two  universities  indicate","metadata":{"id":2}}],["13bfc84b-53e2-4bcc-ad0a-efbd2bb2bd7e",{"pageContent":"quality of online assessments along six dimensions. Responses from \n389  students  and  36  educators across  two  universities  indicate \nmoderate usage  of  generative  AI, consensus  for  which  types  of \nassessments  are  most  impacted,  and  concerns  about  academic \nintegrity. Educators prefer adapted assessments that assume AI will \nbe used and encourage critical thinking, but students’ reaction is \nmixed,  in  part  due  to  concerns  about  a  loss  of  creativity. The \nfindings show the importance of engaging educators and students \nin assessment reform efforts to focus on the process of learning over \nits outputs, higher-order thinking, and authentic applications. \nCCS CONCEPTS \n• Applied computing ~ Education \nKEYWORDS \nAssessment, Generative AI, ChatGPT, Educators, Students, Survey \nACM Reference format: \nAdele Smolansky, Andrew Cram, Corina Raduescu, Sandris Zeivots, Elaine \nHuber and René F. Kizilcec. 2023. Educator and Student Perspectives on the","metadata":{"id":3}}],["ce7526d4-0603-44f7-9cbe-fc4c38c89377",{"pageContent":"ACM Reference format: \nAdele Smolansky, Andrew Cram, Corina Raduescu, Sandris Zeivots, Elaine \nHuber and René F. Kizilcec. 2023. Educator and Student Perspectives on the \nImpact of Generative AI on Assessments in Higher Education. In Proceedings \nof the Tenth ACM Conference on Learning @ Scale (L@S ’23), July 20 1–22, \nCopenhagen,    Denmark. ACM,    New    York,    NY,    USA,    5    pages. \nhttps://doi.org/10.1145/3573051.3596191 \n1 INTRODUCTION \nDespite  the  long  history  of  research and  applications  of Artificial \nIntelligence (AI) in  education, it took  the sudden  popularity  of \nChatGPT for students,  educators,  and  university  administrators \naround  the  world to  be compelled  to grapple  with its practical \nimplications.  The introduction of GPT-3, a large language  model \ndeveloped by OpenAI, and the corresponding ChatGPT tool with a \ngraphical user interface sparked global interest in the potential uses \nand impacts of generative AI in many domains including education.","metadata":{"id":4}}],["f500dd7f-f88f-4c03-ac50-5df0bd57cea6",{"pageContent":"developed by OpenAI, and the corresponding ChatGPT tool with a \ngraphical user interface sparked global interest in the potential uses \nand impacts of generative AI in many domains including education. \nGenerative AI tools can write human-like text in a conversational \nstyle, and ChatGPT has been applied in language translation, chat \nbots  for  conversations  with  humans, writing articles,  stories, \ncomputer code, and other types of writing [1]. \nGenerative AI tools can offer many benefits in education, such \nas  increasing student  engagement in  learning tasks, providing \ntimely feedback, aiding collaboration, and improving accessibility. \nFor  example, the  ability  to  provide  immediate  and  meaningful \nfeedback through automated marking is a key benefit [2]. However, \nAI also raises  serious  concerns about  the  validity  of  assessment \npractices, including issues of academic integrity, especially honesty \nand plagiarism [3]. Researchers have highlighted the potential for","metadata":{"id":5}}],["e51490c2-ef19-434d-8ad0-15b8ee5a92a8",{"pageContent":"practices, including issues of academic integrity, especially honesty \nand plagiarism [3]. Researchers have highlighted the potential for \n \n \nPermission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for  personal  or \nclassroom use is granted without fee provided that copies are not made or distributed for \nprofit or commercial advantage and that copies bear this notice and the full citation on \nthe first page. Copyrights for components of this work owned by others than the author(s) \nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior specific permission and/or a fee. \nRequest permissions from Permissions@acm.org.  \nL@S '23, July 20–22, 2023, Copenhagen, Denmark  \n© 2023 Copyright is held by the owner/author(s). Publication rights licensed to ACM.  \nACM ISBN 979-8-4007-0025-5/23/07...$15.00  \nhttps://doi.org/10.1145/3573051.3596191 \n378","metadata":{"id":6}}],["df1560af-cf37-41e1-816e-575cc59c27d4",{"pageContent":"© 2023 Copyright is held by the owner/author(s). Publication rights licensed to ACM.  \nACM ISBN 979-8-4007-0025-5/23/07...$15.00  \nhttps://doi.org/10.1145/3573051.3596191 \n378\nL@S’23, July, 2023, Copenhagen, Denmark Adele Smolansky et al. \n \n \n \nplagiarism as a key challenge with using ChatGPT for assessment \nin higher education [1]. Students can potentially use generative AI \ntools  like  ChatGPT  to  cheat  on  online  assessments by  submitting \nessays that are not their own work. This can challenge academics’ \nability to distinguish between students’ own work and responses \ngenerated by such tools. It also makes it harder to assess students’ \nlevel   of   understanding   and ability   to   apply material. Unless \neducators  and  academic  institutions  adapt  to  this  new  reality, \ngenerative   AI can undermine academic   integrity   in   online \nassessment   and   the   purpose   of   higher   education   to   educate \nstudents, and ultimately reduce the value of a university degree [1].","metadata":{"id":7}}],["cbb659c9-e6ce-499f-8f32-79da7b371165",{"pageContent":"assessment   and   the   purpose   of   higher   education   to   educate \nstudents, and ultimately reduce the value of a university degree [1]. \nAdapting  current  assessment  practices in  response  to  the \nubiquitous  availability  of generative  AI  tools  is  timely  but  also \neffortful. Researchers  have identified several challenges  educators \nface  in  designing  and  implementing new online assessments [4]. \nThese challenges  include additional  academic  teacher  time  and \neffort,  the  logistics  and  timing  of new kinds  of assessments, \ntechnology   access, consistency over   time,   functionality   and \nusability,  alignment  with  student  preferences  and  expectations, \neffectively preparing students  for new assessment  formats,  and \ninstitutional  and  departmental  policies  that  might  inhibit new \nassessment   designs and   implementations. Considering these \nchallenges,  it  is  important  to  understand  educators’  attitudes","metadata":{"id":8}}],["92c9bb79-9d7f-4880-a0ee-13ace93b15ab",{"pageContent":"assessment   designs and   implementations. Considering these \nchallenges,  it  is  important  to  understand  educators’  attitudes \ntowards generative AI in the context of assessments. Yet, educators \nare not the only ones who will be quick to respond to assessment \nreform efforts. Students have been looking for guidance on how to \napproach assessments that were not designed with the capabilities \nand availability of modern AI tools in mind. \nWhile  this  moment  in  time  presents  a  rare  opportunity  for \ninnovation  in  current  assessment  practices,  it  is  important  to \nunderstand both the educators’ and students’ perspectives on the \nissue  to  achieve  sustainable  improvements. We  developed and \nconducted a survey to compare the attitudes of both stakeholders, \nbuilding on an established framework for examining the quality of \nuniversity assessments.   Educators   and   students   are   asked   to \nconsider several assessment scenarios and indicate their preferences","metadata":{"id":9}}],["9e7673ae-6353-4024-844d-35a136f742de",{"pageContent":"university assessments.   Educators   and   students   are   asked   to \nconsider several assessment scenarios and indicate their preferences \nto help us understand their perceptions of different adaptations and \ninform efforts to reform assessment practices. \n2 RELATED WORK \nIn   a national study of Australian   educators   in   the   business \ndisciplines, researchers investigated the  factors  that  influence  the \ndesign and evaluation of online assessments, which are defined as \nany type of graded activity with an online component designed to \nmeasure students’ mastery of knowledge and/or skills [5]. Building \non an extensive literature review, they identified six design criteria \nand  four  contextual  factors  at  play (Figure  1). The design  criteria \ninclude ensuring academic  integrity,  providing  valuable  feedback, \ncreating  a  positive  learning  experience  for  students, delivering \nauthentic assessment  tasks, maintaining  the  integrity  of  student","metadata":{"id":10}}],["70c01816-c1cb-45b4-b378-f9e5ef9c26f5",{"pageContent":"creating  a  positive  learning  experience  for  students, delivering \nauthentic assessment  tasks, maintaining  the  integrity  of  student \ninformation,  and  ensuring  equal  opportunities  for  all  students  to \ncomplete   the assessment   successfully (Table   1). The   broader \ncontextual  factors that influence  assessment  design  decisions and \npractices include    scale    of    delivery,    resource    constraints, \ninstitutional  policies,  and  accreditation  requirements. Their study \nalso identified constraints and trade-offs that need to be negotiated \nin designing, evaluating, and implementing online assessments; the \nmost important one was academic integrity (see definition in Table \n1). Online  assessment  presents  practical  challenges  for  student \nauthentication   and   academic   integrity: identity   verification   is \nespecially  difficult  in  online  essays, and remote  invigilation is \nchallenging for online examinations [2, 4].","metadata":{"id":11}}],["140bf65d-c797-4074-b97b-f02c2871c25a",{"pageContent":"authentication   and   academic   integrity: identity   verification   is \nespecially  difficult  in  online  essays, and remote  invigilation is \nchallenging for online examinations [2, 4]. \n \n \nFigure  1: A  framework of  design  criteria and contextual \nfactors for quality online assessment [5]. \nIn this research study, we build on a framework [5], which has thus \nfar  been  tested  in  the  Australian  higher  education  context with \ninstructors,  by  testing  it  as  a  heuristic  to  evaluate  the  impact  of \ngenerative AI on assessments in new contexts. In particular, we are \ncollecting  data  in  a  US  higher  education  context  and including \nstudents in the sample. We believe that the framework is likely to \ngeneralize across countries and we are interested in examining if it \ncan accurately reflect students’ perspectives on assessment as well. \nThis study centers both the educator and student perspective to gain \na holistic and comparative understanding of how these two groups","metadata":{"id":12}}],["0f9dace4-f6ca-46c8-8dd9-efc53c6cd1f0",{"pageContent":"This study centers both the educator and student perspective to gain \na holistic and comparative understanding of how these two groups \nmake sense of how recent technological changes affect assessments. \nTable 1: Assessment design dimensions with descriptions as \nprovided in the survey instrument. \nDimension The extent to which the assessment ... \nAcademic \nintegrity \nensures security against cheating, \nimpersonation, and other forms of inappropriate \nassistance \nStudent \nexperience \nenhances convenience and comfort for students, \nmotivation, and concentration, minimizes stress \nand anxiety, and technical complication \nAuthenticity has similar tasks to those performed in \nworkplace or professional settings \nInformation \nintegrity \nreduces the likelihood of privacy breach (i.e., \nunauthorized access to student personal data, \ncontent students generated in their assessments) \nQuality \nfeedback \nenables the provision of quality feedback (e.g., \ntimely, multiple formats such as media, text, \n379","metadata":{"id":13}}],["eb8a098f-f63b-451b-8045-6a7df21a60f3",{"pageContent":"content students generated in their assessments) \nQuality \nfeedback \nenables the provision of quality feedback (e.g., \ntimely, multiple formats such as media, text, \n379\nEducator and Student Perspectives on the Impact of Generative \nAI on Assessments in Higher Education \nL@S’23, July, 2023, Copenhagen, Denmark A. Smolansky et al. \n \n \nencourages the use of feedback towards later \nassessment) \nEquity of \naccess \nenables flexible conditions to complete the \nassessment (e.g., ease of access for students with \ndisability/ impairment, limited access to \ntechnology, geographically dispersed) \n3 METHODS \nWe have  an  ongoing survey  study  to  investigate  student  and \neducator perspectives on the impact of generative AI tools on the \ndesign  and  use  of  online  assessments. The  survey  instrument we \ndeveloped allows   us to evaluate   the   impact   across   various \ndimensions of online assessments [5] and evaluate similarities and \ndifferences between the student and educator perspectives.","metadata":{"id":14}}],["56a6e1f6-3ad2-4690-b705-a0acd6d319fc",{"pageContent":"developed allows   us to evaluate   the   impact   across   various \ndimensions of online assessments [5] and evaluate similarities and \ndifferences between the student and educator perspectives.  \nThe sample we have collected thus far consists of 389 students \nand   36   educators from two selective institutions   of   higher \neducation, one in Australia (338 students; 26 educators) and one in \nthe United States (51 students; 10 educators). We recruited students \nat different stages in their programs and in a variety of disciplines. \nEducators  from  different  departments  with  variable  assessment \npractices were recruited. The  survey took  about 15  minutes  to \ncomplete. It consists  mostly  of  scenario-based  multiple-choice \nquestions. For the U.S. sample, incentives to respond include charity \ndonations and course credit for students; no incentives were offered \nfor  Australian  respondents. Respondents  who provided  informed","metadata":{"id":15}}],["20766520-9e44-4f58-b10e-f44e4f598049",{"pageContent":"donations and course credit for students; no incentives were offered \nfor  Australian  respondents. Respondents  who provided  informed \nconsent  and  answered  the  first  set  of  question were  retained for \nanalysis. Responses were not required to proceed. \nThe  survey  format  and  questions were almost  identical  for \neducators   and   students   since our   goal   is to   compare their \nperspectives. We ask respondents about their past experience with \nChatGPT and other generative  AI tools and their opinions on the \nimpact  of  generative  AI  tools  on eight specific types of  online \nassessment. We then present two sets of assessment scenarios, an \nessay-based assessment and a coding-based assessment: \nEssay assessment prompt: Write a 5-page essay on [a given topic \nin your discipline; e.g., Greek mythology, human rights, sustainable \nenergy, sorting algorithms]. You have 7 days to complete the essay. \nCoding  assessment prompt: (1)  Write  two  different  algorithms","metadata":{"id":16}}],["52a38bd2-8107-48e5-b673-1c57665e8596",{"pageContent":"energy, sorting algorithms]. You have 7 days to complete the essay. \nCoding  assessment prompt: (1)  Write  two  different  algorithms \nusing  Python  code to  sort  a  list  of  numbers.  (2)  Evaluate  the \ncorrectness of each approach (1-2 paragraphs each). (3) Analyze the \ntime complexities (i.e., how long they take depending on the length \nof the list) of each algorithm (1-2 paragraphs each). You have 7 days \nto submit your solution to the above questions. \nAfter reading each assessment prompt, participants were asked: \n“Consider  the  assessment  prompt  prior  to  the  availability  of \ngenerative  AI  tools,  such  as  ChatGPT.  To  what  extent  does  this \nassessment ensure each design assessment dimension (see Table 1). \nNow consider the same assessment prompt again but in today’s \ncontext  when  generative  AI  tools,  such  as  ChatGPT,  are  widely \navailable (i.e., students have equal access to the tool). To what extent","metadata":{"id":17}}],["a11523b0-9cfc-46cc-8a04-b6cd59e8db89",{"pageContent":"context  when  generative  AI  tools,  such  as  ChatGPT,  are  widely \navailable (i.e., students have equal access to the tool). To what extent \ndoes  this  assessment ensure  each  design  assessment  dimension?” \nThey respond for each dimension of the framework in Table 1 using \nthe response options None, Low, Medium, High, Not Sure. Then they \nare  asked  an  open-ended  question: “How  do  you  think  students \nmight use generative AI tools to complete this assessment?” \nNext, participants are given a modified version of the assessment \nprompt and told “consider an adapted version of the assessment \nprompt,  given  today  when  generative  AI  tools,  such  as  ChatGPT, \nare widely available.” \nAdapted essay prompt: You are given a 5-page essay produced by \nChatGPT   on   [a   given   topic   in   your   discipline;   e.g.,   Greek \nmythology, human rights, sustainable energy, sorting algorithms]. \nYou have 7 days to analyze the essay and edit it yourself to improve","metadata":{"id":18}}],["2acdd3dd-d7b1-460d-82d6-54563039afca",{"pageContent":"mythology, human rights, sustainable energy, sorting algorithms]. \nYou have 7 days to analyze the essay and edit it yourself to improve \nits  quality,  making  clear  references  to  the  original  text  where \napplicable. \nAdapted  coding  prompt: Evaluate  and  compare  two  different \nalgorithmic approaches to sort a list of numbers by following these \nthree steps: 1. Ask ChatGPT to generate an algorithm using Python \ncode to sort a list of numbers. Provide the output. In 1-2 paragraphs, \nexplain whether you think the code is correct. Include examples of \nusing the algorithm. 2. Ask  ChatGPT to generate a more efficient \nalgorithm using Python code to sort a list of numbers. Provide the \noutput.  In  1-2  paragraphs,  explain  whether  you  think  the  code  is \ncorrect. Include examples of using the algorithm. 3. Ask ChatGPT \nto analyze the time complexity of the algorithms (i.e., how long they \ntake depending on the length of the list). Provide the output. In 1-2","metadata":{"id":19}}],["b407771c-6ba2-4abc-a6e3-427aba3efef9",{"pageContent":"to analyze the time complexity of the algorithms (i.e., how long they \ntake depending on the length of the list). Provide the output. In 1-2 \nparagraphs, explain whether you agree with the output. You have 7 \ndays to submit responses to the above questions. \nAfter  being  presented  with  each  adapted  assessment  prompt, \nparticipants are asked again: “To what extent does this assessment \nensure each design assessment dimension (Table 1)? This yields a \nclear within-participant comparison of how the adaptation affects \ntheir judgement of assessment quality. In addition, participants are \nasked to choose  between the original prompt, adapted prompt, or \nindicate  no  preference  between  the  two: “Assuming  ChatGPT  is \navailable,   which   assessment   do   you   prefer?” and  “Assuming \nChatGPT  is  available,  which  scenario  do  you  think  students \n(educators) prefer?\"  This  not  only  yields  a  measure  of  their \npreference but also a measure of what they think the other prefers.","metadata":{"id":20}}],["33e43367-310c-4c22-8a7a-1ca1cead3dea",{"pageContent":"(educators) prefer?\"  This  not  only  yields  a  measure  of  their \npreference but also a measure of what they think the other prefers. \nTo  analyze  the  data we  have  collected  so  far,  we compute \npercentages and means of participants’ ratings to observe major \ntrends pooling across the two institutions. We present examples of \ncommon open-ended  responses,  prior  to  conducting  a  complete \nthematic  analysis. This  analysis focuses  on  examining  differences \nbetween student and educator responses.  \n4 PRELIMINARY FINDINGS \nWe find that almost every student and all educators in our sample \nhave heard of ChatGPT. However, only one in four students used it \nweekly or daily for coursework (29% Australia; 24% US) and for fun \n(25% Australia; 14% US). Educators were using it weekly or daily for \nprofessional  purposes  (35%  Australia; 10%  US),  for  research  (15% \nAustralia; 30%  US),  and  for  fun  (31%  Australia;  40%  US).  Besides","metadata":{"id":21}}],["74bf6df5-d037-4519-94b1-97157c182470",{"pageContent":"professional  purposes  (35%  Australia; 10%  US),  for  research  (15% \nAustralia; 30%  US),  and  for  fun  (31%  Australia;  40%  US).  Besides \nChatGPT,  students  and  educators  use  tools  like Anthropic,  Bard, \nBingChat, ClaudeAI, DALL-E, Midjourney, and Stable Diffusion. \nStudents  and  educators  rated  how  much  different  types  of \nassessment are impacted by generative AI and there was consensus \n380\nL@S’23, July, 2023, Copenhagen, Denmark Adele Smolansky et al. \n \n \n \nthat  essays  (incl.  reports,  literature  review,  case  studies,  research \npapers),  computer  code  (incl.  pseudo-code,  mathematical  proofs), \nshort-answer  and multiple-choice  questions  are  very  or  at  least \nmoderately  impacted.  Assessments  that  require  product  design  or \ncreative/artistic  work  are  also  rated  as  moderately  affected.  The \nassignment types rated to be least impacted are presentations and \ndiscussions that   are   either   pre-recorded   or   live. Educators,","metadata":{"id":22}}],["12998efa-9f7f-4d9a-9b40-767039cad80b",{"pageContent":"assignment types rated to be least impacted are presentations and \ndiscussions that   are   either   pre-recorded   or   live. Educators, \nespecially  in  Australia  (Fig.  1), rated the  impact of  generative  AI \nhigher than students for all types of assignments. \n \n \nFigure 2: Australian educators’ and students’ ratings of how \nimpacted different types of assessment are by generative AI. \nMean ratings with standard error bars are shown. \nFor  both  specific  assessment  scenarios (the essay  and  coding \nprompts), students and educators reported a substantial drop in the \noriginal prompt’s capacity to ensure academic integrity due to the \navailability  of  ChatGPT. The  proposed  adaptation  to  the  original \nprompt  was  generally  thought  to  restore  the  level  of  academic \nintegrity. A similar pattern was observed for students’ ratings of the \nassessment’s authenticity.  Yet  ratings  of  the  other  dimensions  of \nassessment  quality  shown  in  Table  1  remained  largely  stable  for","metadata":{"id":23}}],["d9385026-8b36-4b35-a6cd-e78793010ff1",{"pageContent":"assessment’s authenticity.  Yet  ratings  of  the  other  dimensions  of \nassessment  quality  shown  in  Table  1  remained  largely  stable  for \nstudents and educators. \nFinally, the preference ratings for the original compared to the \nadapted  assessment  prompt  by  each  set  of  stakeholders pooled \nacross  countries  is  visualized  in  Figure 3.  It  shows  that  educators \nhave a strong preference for the adapted prompt while students are \nless convinced by the adaptation, especially for the essay prompt.  \nOpen-ended responses from students indicated concern about a \nloss in creativity in the adapted essay prompt because it gave them \nan  essay  to  critique  instead  of  asking  them  to  write  one  on  their \nown: “it kills creativity. You can’t ask humans to be the secretary to \nmachines.” Students also raised concerns about the adapted coding \nassessment: “I would prefer to learn how to write code than learn to \nanalyse” and “[it] replaces  the  teacher  with  ChatGPT,  removing","metadata":{"id":24}}],["65efc422-962b-4625-8715-b4997788442a",{"pageContent":"assessment: “I would prefer to learn how to write code than learn to \nanalyse” and “[it] replaces  the  teacher  with  ChatGPT,  removing \nspace   for   quality   feedback.” Yet students   also indicated   that \nChatGPT is useful to get them started “[as] a prompt or inspiration \nbut we wouldn’t use it directly as a source.”  \nStudents  also had  forward-looking ethical concerns: “Students \nshould be allowed to use generative AI tools like a tool to boost their \nproductivity  as  long  as  they can  be  accepted  in  their  future \nworkplace.” For the coding assessment, students noted that they are \nalready using various tools: “before using AI tools, students have \nbeen using Google or GitHub to search for similar codes, which is \nalso widely accepted in the workplace.” \n \n \nFigure 3: Educators’ and students’ preferences for an original \nor  adapted  assessment  prompt  for  an  essay  and  coding \nscenario. Standard error bars are shown. \n5  DISCUSSION","metadata":{"id":25}}],["f8868be8-ffe1-4f72-a7f7-b959098112df",{"pageContent":"Figure 3: Educators’ and students’ preferences for an original \nor  adapted  assessment  prompt  for  an  essay  and  coding \nscenario. Standard error bars are shown. \n5  DISCUSSION \nThis ongoing  study  uses a  theoretically  grounded  instrument  to \nunderstand  educator  and  student  attitudes  about  the  impact  of \ngenerative  AI  on  assessments. As  we  collect  more  data  in  the \ncoming  months,  we  plan  to  examine  variation  in  attitudes  across \ncontexts, backgrounds, and prior experiences. We expected to find \nvariation  between  educators  and  students  and  across  countries  in \nline with different educational approaches and cultural norms. Thus \nfar,  we  have  observed  a  surprising  level  of  agreement  across \nstakeholders and locals. An important exception is that students are \nmore  hesitant  to  endorse  the  adapted  prompt  than  educators  for \nreasons including a perceived loss in creativity. This highlights an","metadata":{"id":26}}],["5ea86bb4-a7e2-4f05-9500-1783cb97815b",{"pageContent":"more  hesitant  to  endorse  the  adapted  prompt  than  educators  for \nreasons including a perceived loss in creativity. This highlights an \nimportant need for carefully designing and framing new assessment \nprompts in ways that center the process of learning, higher-order \nthinking, and authentic tasks.  \nThere is much work to be done in preparing our students for a \nworld in which AI tools are ubiquitous [6]. Indeed, one student in \nour sample wrote, “I think universities should be more interested in \nteaching us how to use more ways to solve problems”. We concur \nwith  other  researchers  that  there  is  a  need  to  build  students’ \ncapabilities in AI, but more importantly, there is a need to help them \nnavigate   the more   complex   interplay   between   technology, \ncognition, social interaction and values [6].  \nACKNOWLEDGMENTS \nWe  thank our survey  respondents.  This  work  is  supported  by  a \nUSyd-Cornell Global Strategic Partnership Collaboration Award. \n \n381","metadata":{"id":27}}],["7c2d74f5-6e5a-4dea-80f4-92af383d2212",{"pageContent":"cognition, social interaction and values [6].  \nACKNOWLEDGMENTS \nWe  thank our survey  respondents.  This  work  is  supported  by  a \nUSyd-Cornell Global Strategic Partnership Collaboration Award. \n \n381\nEducator and Student Perspectives on the Impact of Generative \nAI on Assessments in Higher Education \nL@S’23, July, 2023, Copenhagen, Denmark A. Smolansky et al. \n \n \n \nREFERENCES \n[1]  Debby  R.  E.  Cotton,  Peter  A.  Cotton,  and  J.  Reuben  Shipway.  2023. \nChatting  and  cheating:  Ensuring  academic  integrity  in  the  era  of  ChatGPT. \nInnovations  in  Education  and  Teaching  International 0,  0  (March  2023),  1–12. \nDOI:https://doi.org/10.1080/14703297.2023.2190148 \n[2]  Karen Mate and Judith Weidenhofer. 2022. Considerations and strategies \nfor  effective  online  assessment  with  a  focus  on  the  biomedical  sciences. FASEB \nBioAdvances 4, 1 (2022), 9–21. DOI:https://doi.org/10.1096/fba.2021-00075 \n[3] Zachari Swiecki, Hassan Khosravi, Guanliang Chen, Roberto Martinez-","metadata":{"id":28}}],["c4e9ebfd-709e-4b96-ac5b-44c85e2c5d7d",{"pageContent":"BioAdvances 4, 1 (2022), 9–21. DOI:https://doi.org/10.1096/fba.2021-00075 \n[3] Zachari Swiecki, Hassan Khosravi, Guanliang Chen, Roberto Martinez-\nMaldonado, Jason M. Lodge, Sandra Milligan, Neil Selwyn, and Dragan Gašević. 2022. \nAssessment in the age of artificial intelligence. Computers and Education: Artificial \nIntelligence 3, (2022), 100075. DOI:https://doi.org/10.1016/j.caeai.2022.100075 \n[4]  Andrew  Cram,  Lynne  Harris,  Corina  Raduescu,  Elaine  Huber,  Sandris \nZeivots, Andrew Brodzeli, Sue Wright, and Amanda White. 2022. Online Assessment \nin  Australian  University  Business  Schools:  A  Snapshot  of  Usage  and  Challenges. \nASCILITE Publications (November 2022), e22181–e22181. \nDOI:https://doi.org/10.14742/apubs.2022.181 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n[5]  Elaine  Huber,  Lynne  Harris,  Sue  Wright,  Amanda  White,  Corina \nRaduescu,  Sandris  Zeivots,  Andrew  Cram,  and  Andrew  Brodzeli.  2023.  Towards  a","metadata":{"id":29}}],["6ce846a9-3f10-4584-89f8-dca0f529f679",{"pageContent":"[5]  Elaine  Huber,  Lynne  Harris,  Sue  Wright,  Amanda  White,  Corina \nRaduescu,  Sandris  Zeivots,  Andrew  Cram,  and  Andrew  Brodzeli.  2023.  Towards  a \nframework for designing and evaluating online assessments in business education. \nAssessment and evaluation in higher education (2023). \nDOI:https://doi.org/10.1080/02602938.2023.2183487 \n[6] Lina   Markauskaite,   Rebecca   Marrone,   Oleksandra   Poquet,   Simon \nKnight, Roberto Martinez-Maldonado, Sarah Howard, Jo Tondeur, Maarten De Laat, \nSimon Buckingham Shum, Dragan Gašević, and George Siemens. 2022. Rethinking \nthe   entwinement   between   artificial   intelligence   and   human   learning:   What \ncapabilities do learners need for a world with AI? Computers and Education: Artificial \nIntelligence 3, (January 2022), 100056. DOI:https://doi.org/10.1016/j.caeai.2022.100056 \n \n382\nView publication stats","metadata":{"id":30}}]],{"0":"0ecc71d7-39f3-4dde-a5b7-f50be50989c5","1":"d2852896-b99a-4d7b-afc3-86777396ef65","2":"d0378b7c-2dec-428b-8c83-53b8c46c36e8","3":"13bfc84b-53e2-4bcc-ad0a-efbd2bb2bd7e","4":"ce7526d4-0603-44f7-9cbe-fc4c38c89377","5":"f500dd7f-f88f-4c03-ac50-5df0bd57cea6","6":"e51490c2-ef19-434d-8ad0-15b8ee5a92a8","7":"df1560af-cf37-41e1-816e-575cc59c27d4","8":"cbb659c9-e6ce-499f-8f32-79da7b371165","9":"92c9bb79-9d7f-4880-a0ee-13ace93b15ab","10":"9e7673ae-6353-4024-844d-35a136f742de","11":"70c01816-c1cb-45b4-b378-f9e5ef9c26f5","12":"140bf65d-c797-4074-b97b-f02c2871c25a","13":"0f9dace4-f6ca-46c8-8dd9-efc53c6cd1f0","14":"eb8a098f-f63b-451b-8045-6a7df21a60f3","15":"56a6e1f6-3ad2-4690-b705-a0acd6d319fc","16":"20766520-9e44-4f58-b10e-f44e4f598049","17":"52a38bd2-8107-48e5-b673-1c57665e8596","18":"a11523b0-9cfc-46cc-8a04-b6cd59e8db89","19":"2acdd3dd-d7b1-460d-82d6-54563039afca","20":"b407771c-6ba2-4abc-a6e3-427aba3efef9","21":"33e43367-310c-4c22-8a7a-1ca1cead3dea","22":"74bf6df5-d037-4519-94b1-97157c182470","23":"12998efa-9f7f-4d9a-9b40-767039cad80b","24":"d9385026-8b36-4b35-a6cd-e78793010ff1","25":"65efc422-962b-4625-8715-b4997788442a","26":"f8868be8-ffe1-4f72-a7f7-b959098112df","27":"5ea86bb4-a7e2-4f05-9500-1783cb97815b","28":"7c2d74f5-6e5a-4dea-80f4-92af383d2212","29":"c4e9ebfd-709e-4b96-ac5b-44c85e2c5d7d","30":"6ce846a9-3f10-4584-89f8-dca0f529f679"}]